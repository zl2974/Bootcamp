---
title: "Instrumental Variable Example"
author: "Jeffrey Zhuohui Liang"
date: "7/25/2021"
output: github_document
---

```{r setup, include=FALSE}
library(MASS)
library(caret)
library(tidyverse)

knitr::opts_chunk$set(echo = TRUE,
                      warning = F,
                      message = F)

options(digits = 4)
```


# Background

  This example is a modification of the example in Textbook Chapter 19 and its code[1].
  
  We want to estimate the effect on ICU mortality of receiving care in a non-target ICU( a ICU unit which have a different specialty focus than ICU to which the patient should be assigned to in the absence of capacity constrains). In our example, we will focus on patient assigned to "MED" medical ICU service whom designated to a non- Medical ICU (MICU), and we define these patients as `boarder`.

  Assumption:  Patient using Medical service admission normally will require a MICU unit. We want to look at the association of using other ICUs instead of MICU for patients who normally require MICU and the probability of death within 24 hours of discharge from ICU or death during ICU stay.
  
  We assume that the staff others than physicians of MICU team will change according to the boarding status. As as result the boarder are typically cared for by nurse and other staffs are more experience with for example surgical patients and not familiar with the procedures of the other wings of the ICU. Besides, communication, geographic distance can delay the care in time from MICU's physicians. It seems nature that boarder will have a negative impact on outcomes.
  
  we assume that the decision to board a patient is not random, i.e. the patient's severity and condition compared to other coming patients is take into accounts. For example, assume that boarder increase mortality, so the physicians decide to board less severe patient and increase the survival of severe patient. But such decision is unobservable. 

# Method

## Data Exploration

  In this example, we filter the patient whom received "MED" services and as such should be assigned to MICU. This help excluding any reason aside from capacity constraints for a patients to be boarder in a non MICU unit (Example from textbook: a postoperative subject in the surgical ICU being transferred to MICU for persistent respiratory failure).

```{r data_reading,eval = F, include=F}
example_df = read_csv("data/example_data.csv") %>% 
  janitor::clean_names()

str(example_df)
```
  
  Whether a patient is cared by a `micu team` is our primary interest predictor, but we assume that there is unobserved confounding that related to both variable and outcome.So the number of `remaining beds` in MICU is defined as our Instrumental Variable.
  
\begin{equation}
Remaining~Beds = (MICU~Capacity - No.of~Inboarders) - (Team~Census - No. of~ Boarders)
\end{equation}


  where `inboarders` that represent a MICU bed occupied by a non-MICU patient at the time a MICU patient began their
ICU stay and we also determine whether the new MICU patient was assigned a bed outside the geographic confines of the MICU, in which case they were classified as a `boarder`.
  
  Our instrument is controlled for `team census`(how many patients were assigned to the MICU) size since there is an intuitive inverse relationship between team census size and the number of remaining beds, and it is conceivable that team census size could affect the outcome.
  
  We can see that the actual remaining bed for Medical service patient is the `capacity` - `no.of.inboarders`, and those whom actually designated to MICU is the `patient coming(Team census)` - `no.of.boarders`.

We also control for :
 
\begin{itemize}
 
 \item Age
 \item Gender
 \item OASIS score
 \item team census
 \item some elixhauser related terms
 \item number of previously icustay
 \item icd9 code
 
\end{itemize}
 
 and our outcome is expire `within_24_hour` of callout or during ICU stay.
 
 

```{r data_cleansing}
example_df = read_csv("data/example_data.csv") %>% 
  janitor::clean_names() %>% 
  select(
    micu_team,
    age,
    gender,
    oasis,
    remaining_beds,
    team_census,
    congestive_heart_failure:depression,
    num_icustay,
    icd9_code,
    within_24_hours
  ) %>%
  mutate(across(
    c(
      gender,
      congestive_heart_failure:depression,
      within_24_hours,
      micu_team,
      icd9_code
    ),
    as.factor
  ),
  age = case_when(age<100 ~ age,
                  age >= 100 ~ 89)) %>% # change integer to factor
  select_if(function(x)
    length(unique(x)) > 1)   # drop factors that have only 1 value


drop_col = example_df %>%  
  select(where(is.factor),-icd9_code) %>%
  apply(., 2, function(x)
    min(plyr::count(x)$freq)) %>% 
  .[.<=30] %>% 
  names()
# Combine condition that have less than 10 obervation in group into a combined group
example_df["other_condition"] = as.factor(apply(example_df[, drop_col], 1, function(x)
  max(as.numeric(x))))

# drop factors that contains less than 10 observation per groups
example_df = example_df[,!colnames(example_df) %in% drop_col]

example_df = example_df %>% 
  group_by(icd9_code) %>% 
  filter(n()>=5) %>% # we filter out drg group which has less than 10 observation
  ungroup()

  
skimr::skim_without_charts(example_df) # statistics of our example data
```


```{r data_exploration}
# test the association of outcome and variable of interest
chisq.test(example_df$within_24_hours,example_df$micu_team) 

# test the association of intrumental variable and variable of interest
t.test(formula = remaining_beds~micu_team,data = example_df)

# test the assocaition of intrumental variable and outcome
t.test(formula = remaining_beds~within_24_hours,data = example_df)

corrplot::corrplot(
  cor(example_df %>% select(where(is.numeric)))
)

featurePlot(
  x = model.matrix(within_24_hours ~ .-icd9_code, example_df %>% select(where(is.factor)))[,-1],
  y = example_df$within_24_hours,
  scales = list(
    x = list(relation = "free"),
    y = list(relation = "free")
  ),
  plot = "density",
  auto.key = list(columns = 2.5)
)
```

## Model analysis


```{r model_analysis}
### "Bias" estimation

md_bias = glm(within_24_hours ~ .-icd9_code,
              family = "binomial",
              data = example_df[,!colnames(example_df) %in% "remaining_beds"])

broom::tidy(md_bias) %>%
  head() %>%
  knitr::kable(digits = 4,
               caption = "Biased Model Log Odd ratio Estimatiom")
```


  The coefficient we estimate is Logarithm of Odd Ratio(OR), so we need to apply exponential if we want to inference on OR instead of Log Odd.


```{r odd_ratio}
broom::tidy(md_bias) %>%
  mutate(
    lower = exp(estimate + qnorm(0.025) * std.error),
    upper = exp(estimate + qnorm(0.975) * std.error),
        estimate = exp(estimate)
  ) %>%
  relocate(term, lower, estimate, upper) %>% 
  head() %>% 
  knitr::kable(digits = 4,
               caption = "Biased Model Odd ratio Estimatiom")
```


## Instumental Analysis

  In economics analysis, 2-stage-least-square(2SLS) is used to estimate the adjusted effects of endogenuous variables. But the estimation is different if we consider categorical outcome and predictors. Detailed of estimation method is listed in the reference[2]. We will use `ivtools` packages[3] for this example.


```{r ivtools}
confounders = paste0(
  colnames(example_df)[!colnames(example_df) %in% c("micu_team","remaining_beds","within_24_hours","icd9_code")],
  collapse = "+")

stage_1  = as.formula(
  paste0("micu_team~remaining_beds+",confounders)
)

stage_2 = as.formula(
  paste0("within_24_hours~micu_team+",confounders)
)

md_ivtools =
  ivtools::ivglm(
    estmethod = "ts",
    fitX.LZ = glm(stage_1, family = "binomial", data = example_df),
    fitY.LX = glm(stage_2,
                  family = "binomial",
                  data = example_df),
    data = example_df
  )

bind_cols(rownames(summary(md_ivtools)$coeff), as_tibble(summary(md_ivtools)$coeff))  %>%
  rename(term = `...1`) %>% 
  janitor::clean_names() %>% 
  mutate(
    lower = exp(estimate + qnorm(0.025) * std_error),
    upper = exp(estimate + qnorm(0.975) * std_error),
    estimate = exp(estimate)
  ) %>%
  relocate(term, lower, estimate, upper) %>%
  head() %>% 
  knitr::kable(digits = 4,
               caption = "Intrumental Variable Model Odd ratio Estimatiom")
```



## Sparse Matrix

```{r sparse}
library(Matrix)

confounders = paste0(
  colnames(example_df)[!colnames(example_df) %in% c("micu_team","remaining_beds","within_24_hours")],
  collapse = "+")

stage_1  = as.formula(
  paste0("micu_team~remaining_beds+",confounders)
)

stage_2 = as.formula(
  paste0("within_24_hours~micu_team+",confounders,"+res")
)

### Bias Estimate
train_matrix = sparse.model.matrix(as.formula(paste0(
  "within_24_hours~micu_team+", confounders
)),
data = example_df)[, -1]

dim(train_matrix)

glm_sparse_bias = glmnet::glmnet(
  x = train_matrix,
  y = example_df$within_24_hours,
  family = "binomial",
  nlambda = 1,
  type.logistic = "Newton",
  lambda = 0,
  maxit = 1e+4
)

broom::tidy(glm_sparse_bias) %>% 
  head(n=10) %>% 
  summarise(term = term,
            OR = exp(estimate)) %>% 
  knitr::kable(caption = "Sparse Matrix GLM- Bias estimate")


### First Stage
train_matrix = sparse.model.matrix(stage_1,
                                   data = example_df)[, -1]

dim(train_matrix)

glm_sparse_sg1 = glmnet::glmnet(
  x = train_matrix,
  y = example_df$within_24_hours,
  family = "binomial",
  nlambda = 1,
  type.logistic = "Newton",
  lambda = 0,
  maxit = 1e+4
)

res_sg1 = predict(glm_sparse_sg1,newx = train_matrix,type = "response")

res_sparse_sg1 = as.numeric(example_df$micu_team)- 1 - res_sg1

### Second Stage
stage_2_df = example_df

stage_2_df[, "res"] = res_sparse_sg1

glm_sparse_sg2 = glmnet::glmnet(
  x = sparse.model.matrix(stage_2, data = stage_2_df)[, -1],
  y = stage_2_df$within_24_hours,
  family = "binomial",
  nlambda = 1,
  type.logistic = "Newton",
  lambda = 0,
  maxit = 1e+4
)

broom::tidy(glm_sparse_sg2) %>% 
  head() %>% 
  summarise(term = term,
            OR = exp(estimate)) %>% 
  knitr::kable(caption = "Sparse Matrix GLM-IV estimate")


### Bootstrap estimate

IV_coef = list()

for(i in 1:100){
  train_matrix = sample_n(stage_2_df,
                        size = nrow(stage_2_df),
                        replace = T)
  
  glm_sparse_sg2 = glmnet::glmnet(
    x = sparse.model.matrix(stage_2,data = train_matrix)[,-1],
    y = train_matrix$within_24_hours,
    family = "binomial",
    nlambda = 1,
    type.logistic = "Newton",
    lambda = 0,
    maxit = 1e+4
  )
  
  IV_coef[[i]] = broom::tidy(glm_sparse_sg2)
}

IV_coef = do.call(bind_rows,IV_coef)

IV_coef %>% 
  filter(term %in% head(.)$term) %>% 
  group_by(term) %>% 
  summarise(bootstrap_lwr = exp(quantile(estimate,0.025)),
            bootstrap_upr = exp(quantile(estimate,0.975)),
            bootstrap_estimate = exp(mean(estimate)))  %>% 
  knitr::kable(caption = "Sparse Matrix bootstrap GLM-IV estimate")
```


```{r spark,eval=F,include=F}
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
  Sys.setenv(SPARK_HOME = "/opt/spark")
}
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "4g"))

example_spark = as.DataFrame(example_df)

spark_sg1 = spark.logit(example_spark,within_24_hours~.,regParam = 0,maxIter = 10)

res = predict(spark_sg1,example_spark)
```


  In this example, we can see that after adjusting unobserved confounding in our variable of interest, the effect of variable of interest became not statistically significant, compared to a statistically significant protective effect in the unadjust model.

# Reference


[1] https://github.com/MIT-LCP/critical-data-book/tree/master/part_iii/chapter_19

[2] http://www-stat.wharton.upenn.edu/~dsmall/BCai2009_SIM_Final.pdf

[3] https://cran.r-project.org/web/packages/ivtools/ivtools.pdf